{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n",
    "\n",
    "In case you are facing issues, please read the following two documents first:\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/Environment-Setup\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/FAQ\n",
    "\n",
    "Then, please feel free to ask in the discussion forum of the course:\n",
    "\n",
    "Please make sure to follow the guidelines before asking a question:\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me\n",
    "\n",
    "If running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "\n",
    "if ('sc' in locals() or 'sc' in globals()):\n",
    "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a local spark context (sc) and session (spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pull the data in raw format from the source (github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf HMP_Dataset\n",
    "!git clone https://github.com/wchill/HMP_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data set contains data in raw text format. For each category one folde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls HMP_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls HMP_Dataset/Brush_teeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ./HMP_Dataset/Brush_teeth/Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each file contains three columns of integer accelerometer readings as a time series, lets create the appropriate schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"x\", IntegerType(), True),\n",
    "    StructField(\"y\", IntegerType(), True),\n",
    "    StructField(\"z\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes a while, it parses through all files and folders and creates a temporary dataframe for each file which gets appended to an overall data-frame \"df\". In addition, a column called \"class\" is added to allow for straightforward usage in Spark afterwards in a supervised machine learning scenario for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "d = 'HMP_Dataset/'\n",
    "\n",
    "# filter list for all folders containing data (folders that don't start with .)\n",
    "file_list_filtered = [s for s in os.listdir(d) if os.path.isdir(os.path.join(d,s)) & ~fnmatch.fnmatch(s, '.*')]\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#create pandas data frame for all the data\n",
    "\n",
    "df = None\n",
    "\n",
    "for category in file_list_filtered:\n",
    "    data_files = os.listdir('HMP_Dataset/'+category)\n",
    "    \n",
    "    #create a temporary pandas data frame for each data file\n",
    "    for data_file in data_files:\n",
    "        print(data_file)\n",
    "        temp_df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \" \").csv('HMP_Dataset/'+category+'/'+data_file,schema=schema)\n",
    "        \n",
    "        #create a column called \"source\" storing the current CSV file\n",
    "        temp_df = temp_df.withColumn(\"source\", lit(data_file))\n",
    "        \n",
    "        #create a column called \"class\" storing the current data folder\n",
    "        temp_df = temp_df.withColumn(\"class\", lit(category))\n",
    "        \n",
    "        #append to existing data frame list\n",
    "        #data_frames = data_frames + [temp_df]\n",
    "                                                                                                             \n",
    "        if df is None:\n",
    "            df = temp_df\n",
    "        else:\n",
    "            df = df.union(temp_df)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write the dataf-rame to a file in \"parquet\" format, this will also take quite some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('hmp.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have a file with our contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "Please use the data-frame \"df\" below to anser the following questions about the data-frame\n",
    "(you can use SQL or the data-frame api or combine both)\n",
    "\n",
    "Please use the pyspark API doc for your reference. https://spark.apache.org/docs/latest/api/python/reference/index.html\n",
    "\n",
    "\n",
    "\n",
    "1. How many total rows does the data-frame have? (Hint: If you don’t use SQL, there is a single function you can call on the “df” object which returns the solution)\n",
    "\n",
    "2. How many rows in class \"Brush_teeth\"? (Hint: You need to filter first for class=\"Brush_teeth\" before you apply the same function as in question one)\n",
    "\n",
    "3. Which two additional columns beside x, y and z does the data-frame have? (Hint: You can either look at the ETL code from the previous cells or use a field of the “df” object which you can find when looking at the API reference)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "\n",
    "select # your code here # from df where # your code here #\n",
    "\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
