{"nbformat_minor": 1, "cells": [{"source": "This notebook is based on a [blog](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) Francois Chollet has written mid 2016, please refer to that one of more information. In addition, I recommend reading [this](http://adventuresinmachinelearning.com/word2vec-keras-tutorial/) tutorial before to get started on word2vec\n\n\n\nFirst we'll download and exract the necessary files; the glove embeddings on the wikipedia corpus from [Stanford](https://nlp.stanford.edu/projects/glove/) as well as [newsgroup messages](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html) used for classification", "cell_type": "markdown", "metadata": {}}, {"source": "!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip\n!wget http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n!tar xvfz news20.tar.gz", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "COPYRIGHT\n\nAll contributions by Fran\u00e7ois Chollet:\nCopyright (c) 2015 - 2018, Fran\u00e7ois Chollet.\nAll rights reserved.\n\nAll contributions by Google:\nCopyright (c) 2015 - 2018, Google, Inc.\nAll rights reserved.\n\nAll contributions by Microsoft:\nCopyright (c) 2017 - 2018, Microsoft, Inc.\nAll rights reserved.\n\nAll other contributions:\nCopyright (c) 2015 - 2018, the respective contributors.\nAll rights reserved.\n\nEach contributor holds copyright over their respective contributions.\nThe project versioning (Git) records all such contribution source information.\n\nLICENSE\n\nThe MIT License (MIT)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n", "cell_type": "raw", "metadata": {}}, {"source": "source taken from https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py", "cell_type": "markdown", "metadata": {}}, {"source": "from __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model", "cell_type": "code", "execution_count": 1, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/gpfs/fs01/user/se2e-1a118c4f322670-980ba6aaa6c3/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n"}], "metadata": {}}, {"source": "MAX_SEQUENCE_LENGTH = 1000\nMAX_NUM_WORDS = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2", "cell_type": "code", "execution_count": 2, "outputs": [], "metadata": {}}, {"source": "first, build index mapping words in the embeddings set to their embedding vector", "cell_type": "markdown", "metadata": {}}, {"source": "print('Indexing word vectors.')\n\nembeddings_index = {}\nwith open(os.path.join('glove.6B.100d.txt')) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n        \nprint('Found %s word vectors.' % len(embeddings_index))", "cell_type": "code", "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Indexing word vectors.\nFound 400000 word vectors.\n"}], "metadata": {}}, {"source": "prepare text samples and their labels", "cell_type": "markdown", "metadata": {}}, {"source": "print('Processing text dataset')\n\ntexts = []  # list of text samples\nlabels_index = {}  # dictionary mapping label name to numeric id\nlabels = []  # list of label ids\nfor name in sorted(os.listdir('20_newsgroup')):\n    path = os.path.join('20_newsgroup', name)\n    if os.path.isdir(path):\n        label_id = len(labels_index)\n        labels_index[name] = label_id\n        for fname in sorted(os.listdir(path)):\n            if fname.isdigit():\n                fpath = os.path.join(path, fname)\n                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n                with open(fpath, **args) as f:\n                    t = f.read()\n                    i = t.find('\\n\\n')  # skip header\n                    if 0 < i:\n                        t = t[i:]\n                    texts.append(t)\n                labels.append(label_id)\n\nprint('Found %s texts.' % len(texts))", "cell_type": "code", "execution_count": 6, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Processing text dataset\nFound 19997 texts.\n"}], "metadata": {}}, {"source": "vectorize the text samples into a 2D integer tensor", "cell_type": "markdown", "metadata": {}}, {"source": "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))", "cell_type": "code", "execution_count": 11, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 174105 unique tokens.\n"}], "metadata": {}}, {"source": "print(len(sequences))\nprint(len(sequences[0]))\nprint(len(sequences[1]))", "cell_type": "code", "execution_count": 12, "outputs": [{"output_type": "stream", "name": "stdout", "text": "19997\n1528\n5116\n"}], "metadata": {}}, {"source": "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)", "cell_type": "code", "execution_count": 13, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Shape of data tensor: (19997, 1000)\nShape of label tensor: (19997, 20)\n"}], "metadata": {}}, {"source": "split the data into a training set and a validation set , first, shuffle data", "cell_type": "markdown", "metadata": {}}, {"source": "indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]", "cell_type": "code", "execution_count": 17, "outputs": [], "metadata": {}}, {"source": "then split based on split percentage stored in VALIDATION_SPLIT", "cell_type": "markdown", "metadata": {}}, {"source": "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\nx_train = data[:-num_validation_samples]\ny_train = labels[:-num_validation_samples]\nx_val = data[-num_validation_samples:]\ny_val = labels[-num_validation_samples:]", "cell_type": "code", "execution_count": 18, "outputs": [], "metadata": {}}, {"source": "print('Preparing embedding matrix.')\n\n# prepare embedding matrix\nnum_words = min(MAX_NUM_WORDS, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i >= MAX_NUM_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\n\n#Untrained Embedding:\n#embedding_layer = Embedding(len(word_index) + 1,\n#                            EMBEDDING_DIM,\n#                            input_length=MAX_SEQUENCE_LENGTH)", "cell_type": "code", "execution_count": 11, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Preparing embedding matrix.\n"}], "metadata": {}}, {"source": "embeddings_index.get('the')", "cell_type": "code", "execution_count": 41, "outputs": [{"output_type": "execute_result", "metadata": {}, "data": {"text/plain": "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"}, "execution_count": 41}], "metadata": {}}, {"source": "\n\nprint('Training model.')\n\n# train a 1D convnet with global maxpooling\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = GlobalMaxPooling1D()(embedded_sequences)\nx = Dense(128, activation='relu')(x)\npreds = Dense(len(labels_index), activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\nmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=30,\n          validation_data=(x_val, y_val))\n\nprint(\"Training done\")\n", "cell_type": "code", "execution_count": 12, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Training model.\nTrain on 15998 samples, validate on 3999 samples\nEpoch 1/30\n15998/15998 [==============================] - 81s 5ms/step - loss: 2.9310 - acc: 0.1970 - val_loss: 2.7595 - val_acc: 0.3701\nEpoch 2/30\n15998/15998 [==============================] - 82s 5ms/step - loss: 2.2948 - acc: 0.4995 - val_loss: 1.8832 - val_acc: 0.5699\nEpoch 3/30\n15998/15998 [==============================] - 81s 5ms/step - loss: 1.4982 - acc: 0.6572 - val_loss: 1.3274 - val_acc: 0.6569\nEpoch 4/30\n15998/15998 [==============================] - 80s 5ms/step - loss: 1.0370 - acc: 0.7355 - val_loss: 1.0437 - val_acc: 0.7034\nEpoch 5/30\n15998/15998 [==============================] - 81s 5ms/step - loss: 0.7835 - acc: 0.7878 - val_loss: 0.8983 - val_acc: 0.7344\nEpoch 6/30\n15998/15998 [==============================] - 76s 5ms/step - loss: 0.6242 - acc: 0.8295 - val_loss: 0.8134 - val_acc: 0.7539\nEpoch 7/30\n15998/15998 [==============================] - 72s 4ms/step - loss: 0.5095 - acc: 0.8579 - val_loss: 0.7555 - val_acc: 0.7699\nEpoch 8/30\n15998/15998 [==============================] - 74s 5ms/step - loss: 0.4232 - acc: 0.8815 - val_loss: 0.7231 - val_acc: 0.7722\nEpoch 9/30\n15998/15998 [==============================] - 84s 5ms/step - loss: 0.3553 - acc: 0.8992 - val_loss: 0.6949 - val_acc: 0.7824\nEpoch 10/30\n15998/15998 [==============================] - 78s 5ms/step - loss: 0.3001 - acc: 0.9159 - val_loss: 0.6829 - val_acc: 0.7917\nEpoch 11/30\n15998/15998 [==============================] - 78s 5ms/step - loss: 0.2548 - acc: 0.9247 - val_loss: 0.6690 - val_acc: 0.7949\nEpoch 12/30\n15998/15998 [==============================] - 80s 5ms/step - loss: 0.2173 - acc: 0.9350 - val_loss: 0.6732 - val_acc: 0.7934\nEpoch 13/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.1877 - acc: 0.9433 - val_loss: 0.6694 - val_acc: 0.7974\nEpoch 14/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.1629 - acc: 0.9506 - val_loss: 0.6788 - val_acc: 0.7949\nEpoch 15/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.1418 - acc: 0.9556 - val_loss: 0.6975 - val_acc: 0.7919\nEpoch 16/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.1256 - acc: 0.9604 - val_loss: 0.6932 - val_acc: 0.7982\nEpoch 17/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.1109 - acc: 0.9637 - val_loss: 0.7044 - val_acc: 0.7984\nEpoch 18/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.1013 - acc: 0.9663 - val_loss: 0.7176 - val_acc: 0.7984\nEpoch 19/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.0921 - acc: 0.9668 - val_loss: 0.7280 - val_acc: 0.7957\nEpoch 20/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.0851 - acc: 0.9678 - val_loss: 0.7596 - val_acc: 0.7947\nEpoch 21/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.0789 - acc: 0.9682 - val_loss: 0.7625 - val_acc: 0.7977\nEpoch 22/30\n15998/15998 [==============================] - 79s 5ms/step - loss: 0.0743 - acc: 0.9691 - val_loss: 0.7618 - val_acc: 0.7984\nEpoch 23/30\n15998/15998 [==============================] - 81s 5ms/step - loss: 0.0700 - acc: 0.9699 - val_loss: 0.7972 - val_acc: 0.7977\nEpoch 24/30\n15998/15998 [==============================] - 89s 6ms/step - loss: 0.0675 - acc: 0.9701 - val_loss: 0.7991 - val_acc: 0.7982\nEpoch 25/30\n15998/15998 [==============================] - 92s 6ms/step - loss: 0.0646 - acc: 0.9700 - val_loss: 0.8121 - val_acc: 0.7989\nEpoch 26/30\n15998/15998 [==============================] - 90s 6ms/step - loss: 0.0628 - acc: 0.9710 - val_loss: 0.8266 - val_acc: 0.7982\nEpoch 27/30\n15998/15998 [==============================] - 89s 6ms/step - loss: 0.0607 - acc: 0.9701 - val_loss: 0.8435 - val_acc: 0.7972\nEpoch 28/30\n15998/15998 [==============================] - 94s 6ms/step - loss: 0.0595 - acc: 0.9709 - val_loss: 0.8533 - val_acc: 0.7999\nEpoch 29/30\n15998/15998 [==============================] - 92s 6ms/step - loss: 0.0587 - acc: 0.9707 - val_loss: 0.8695 - val_acc: 0.7949\nEpoch 30/30\n15998/15998 [==============================] - 81s 5ms/step - loss: 0.0571 - acc: 0.9701 - val_loss: 0.8643 - val_acc: 0.7974\nTrainin done\n"}], "metadata": {}}, {"source": "preds = model.predict(x_val)", "cell_type": "code", "execution_count": 13, "outputs": [], "metadata": {}}, {"source": "np.sum(np.argmax(preds,axis=1) == np.argmax(y_val,axis=1)) / float(len(preds))\n", "cell_type": "code", "execution_count": 14, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "0.7974493623405852"}, "execution_count": 14, "metadata": {}}], "metadata": {}}, {"source": "x_train", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "x_train.shape", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "y_train.shape", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "def loadGloveModel(gloveFile):\n    print (\"Loading Glove Model\")\n    f = open(gloveFile,'r')\n    model = {}\n    for line in f:\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print (\"Done.\",len(model),\" words loaded!\")\n    return model", "cell_type": "code", "execution_count": 19, "outputs": [], "metadata": {}}, {"source": "model = loadGloveModel('glove.6B.100d.txt')", "cell_type": "code", "execution_count": 20, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Loading Glove Model\nDone. 400000  words loaded!\n"}], "metadata": {}}, {"source": "concept_of_queen = model['king']-model['man']+model['woman']\nnp.sum((concept_of_queen-model['queen'])**2)", "cell_type": "code", "execution_count": 22, "outputs": [{"output_type": "execute_result", "metadata": {}, "data": {"text/plain": "16.65520237287384"}, "execution_count": 22}], "metadata": {}}, {"source": "concept_of_problems = model['iraq']-model['war']\nnp.sum((concept_of_problems-model['iran'])**2)", "cell_type": "code", "execution_count": 25, "outputs": [{"output_type": "execute_result", "metadata": {}, "data": {"text/plain": "45.17898720772049"}, "execution_count": 25}], "metadata": {}}, {"source": "", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.1", "name": "python2-spark21", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "pygments_lexer": "ipython2", "file_extension": ".py", "codemirror_mode": {"version": 2, "name": "ipython"}}}}