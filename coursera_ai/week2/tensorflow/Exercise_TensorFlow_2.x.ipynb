{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to the Exercise on TensorFlow 2.x. \n",
    "\n",
    "First we make us familiar with linear algebra operations. Then we implement a linear regression model. After that we implement a neural network using the low level TensorFlow API. Finally, we conclude with a neural network implemented in Keras.\n",
    "\n",
    "Please replace all ###your_code_here### sections with the correct code.\n",
    "\n",
    "Now let's make sure we are on the latest version of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around with some linear algebra examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1., 2., 2., 3.])\n",
    "print(a)\n",
    "\n",
    "\n",
    "b = tf.constant([4., 5., 5., 6.])\n",
    "print(b)\n",
    "\n",
    "\n",
    "c =  ###your_code_here### => Please add element-wise addition, you should get tf.Tensor([5. 7. 7. 9.]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1., 2., 2., 3.])\n",
    "print(a)\n",
    "\n",
    "\n",
    "b = tf.constant([4., 5., 5., 6.])\n",
    "print(b)\n",
    "\n",
    "\n",
    "c = ###your_code_here### => Please compute the dot product, you should get 42\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "import numpy as np\n",
    "data = np.array(\n",
    "    [\n",
    "        [100,35,35,12,0.32],\n",
    "        [101,46,35,21,0.34],\n",
    "        [130,56,46,3412,12.42],\n",
    "        [131,58,48,3542,13.43]\n",
    "    ]\n",
    ")\n",
    "\n",
    "x = data[:,1:-1]\n",
    "y_target = data[:,-1]\n",
    "\n",
    "b = tf.Variable(1,dtype=tf.float64)\n",
    "w = tf.Variable([1,1,1],dtype=tf.float64)\n",
    "\n",
    "\n",
    "def linear_model(x):\n",
    "    return ###your_code_here### => Please express the linear model using the TensorFlow low level API\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted = linear_model(x)   \n",
    "        loss_value = loss_object(y, predicted)\n",
    "        print ('Loss {} '.format(loss_value))\n",
    "        grads = tape.gradient(loss_value, [b,w])\n",
    "        optimizer.apply_gradients(zip(grads, [b,w]))\n",
    "    \n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "            train_step(x, y_target)\n",
    "    print ('Epoch {} finished'.format(epoch))\n",
    "    \n",
    "train(epochs = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.array(\n",
    "    [\n",
    "        [100,35,35,12,0.],\n",
    "        [101,46,35,21,0.],\n",
    "        [130,56,46,3412,1.],\n",
    "        [131,58,48,3542,1.]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "x = data[:,1:-1]\n",
    "y_target = data[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "x = x / np.linalg.norm(x)\n",
    "\n",
    "b = tf.Variable(1,dtype=tf.float64)\n",
    "w = tf.Variable([1,1,1],dtype=tf.float64)\n",
    "\n",
    "\n",
    "def logstic_model(x):\n",
    "    return ###your_code_here### => Please express the logistic regression model using the TensorFlow low level API\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=10)\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted = logstic_model(x) \n",
    "        loss_value = loss_object(y, predicted)\n",
    "        print(loss_value)\n",
    "        grads = tape.gradient(loss_value, [b,w])\n",
    "        optimizer.apply_gradients(zip(grads, [b,w]))\n",
    "    \n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "            train_step(x, y_target)\n",
    "    \n",
    "train(epochs = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logstic_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "import numpy as np\n",
    "data = np.array(\n",
    "    [\n",
    "        [100,35,35,12,0.32],\n",
    "        [101,46,35,21,0.34],\n",
    "        [130,56,46,3412,12.42],\n",
    "        [131,58,48,3542,13.43]\n",
    "    ]\n",
    ")\n",
    "\n",
    "x = data[:,1:-1]\n",
    "x = x / np.linalg.norm(x)\n",
    "y_target = data[:,-1]\n",
    "y_target = y_target / np.linalg.norm(y_target)\n",
    "\n",
    "\n",
    "w1 = tf.Variable([[1,1,1],[1,1,1],[1,1,1]],dtype=tf.float64)\n",
    "w2 = tf.Variable([1,1,1],dtype=tf.float64)\n",
    "\n",
    "def layer1(x):\n",
    "    return ###your_code_here### => Please express fully connected layer1 with sigmoid activation using the TensorFlow low level API\n",
    "\n",
    "print(layer1(x))\n",
    "\n",
    "def layer2(x):\n",
    "    return ###your_code_here### => Please express fully connected layer2 with sigmoid activation using the TensorFlow low level API\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_object = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted = layer2(x)   \n",
    "        loss_value = loss_object(y, predicted)\n",
    "        print(loss_value)\n",
    "        grads = tape.gradient(loss_value, [w1,w2])\n",
    "        optimizer.apply_gradients(zip(grads, [w1,w2]))\n",
    "    \n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "            train_step(x, y_target)\n",
    "    print ('Epoch {} finished'.format(epoch))\n",
    "    \n",
    "train(epochs = 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "data = np.array(\n",
    "    [\n",
    "        [100,35,35,12,0],\n",
    "        [101,46,35,21,0],\n",
    "        [130,56,46,3412,1],\n",
    "        [131,58,48,3542,1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "x = data[:,1:-1]\n",
    "y_target = data[:,-1]\n",
    "\n",
    "x = x / np.linalg.norm(x)\n",
    "\n",
    "model = Sequential()\n",
    "###your_code_here### => Please express two fully connected layes with sigmoid activation using the Keras high level API\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.1),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x, y_target, epochs=1000,\n",
    "          verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
