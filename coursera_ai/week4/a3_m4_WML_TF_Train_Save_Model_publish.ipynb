{"nbformat_minor": 1, "cells": [{"source": "<html>\n<body>\n    <table style=\"border: none\" align=\"center\">\n        <tr style=\"border: none\">\n            <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"45\" width=\"45\"></th>\n            <th style=\"border: none\"><font face=\"verdana\" size=\"6\" color=\"black\"><b>Watson Machine Learning</b></font></th>\n        </tr>\n    </table>\n</body>\n</html>", "cell_type": "markdown", "metadata": {}}, {"source": "# WML - Tensorflow Support", "cell_type": "markdown", "metadata": {}}, {"source": "## Contens:\n1. Train a Tensorflow model in ** DSX Notebook **\n2. Save the trained model into WML Repository", "cell_type": "markdown", "metadata": {}}, {"source": "## Support Requirements:", "cell_type": "markdown", "metadata": {}}, {"source": "### Supported Versions:\n1. Python Runtime: Python 3.5\n2. Tensorflow version: 1.2 \n3. Anaconda Runtime Version: Anaconda 4.2.9 for Python 3.5", "cell_type": "markdown", "metadata": {}}, {"source": "### Mandatory Requirements for Online Deployment and Scoring:\n1. The model to be deployed using Online Deployment and Scoring service should be persisted in WML Repository \n2. The persisted model should contain Tensorflow signature metadata for serving(Refer section 2.1). Online deployment is restricted for those persisted Tensorflow models that do not contain this meta data. This requirement will be adequately documented.\n", "cell_type": "markdown", "metadata": {}}, {"source": "<hr>", "cell_type": "markdown", "metadata": {}}, {"source": "<table style=\"border: none\" align=\"center\">\n   <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/raw/master/scikit-learn/hand-written-digits-recognition/images/numbers_banner-04.png\" width=\"600\" alt=\"Icon\"> </th>\n   </tr>\n</table>", "cell_type": "markdown", "metadata": {}}, {"source": "# About the use case:- Recognition of hand written digits\nUsing Tensorflow, we train a model that can recognize a handwritten number embedded in an image. The model is trained using the MNIST data set that can be accessed using the Tensorflow's sample dataset related APIs. Here, we use Tensorflow's implementation of Convolutional Neural Network(CNN) to build the model. ", "cell_type": "markdown", "metadata": {}}, {"source": "import tensorflow as tf", "cell_type": "code", "execution_count": 1, "outputs": [], "metadata": {}}, {"source": "# 1.0 Train a Tensorflow model ", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.1 Import training data", "cell_type": "markdown", "metadata": {}}, {"source": "Using the code below, let us download the datasets that we will use for training, validation and test purposes. The APIs used here provides us the datasets in form of a **single dimensional NumPy array** which has been transformed from the actual images. ", "cell_type": "markdown", "metadata": {}}, {"source": "from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)", "cell_type": "code", "execution_count": 2, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Extracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"}], "metadata": {}}, {"source": "### 1.2 Set parameters required for creating the input and the target(label) Tensors", "cell_type": "markdown", "metadata": {}}, {"source": "n_input - Refers to the size of the single dimensional array that represents one image containing hand written number <br>\nn_classes - Refers to the number of possible categories of prediction outcomes. In this use case, the prediction outcome can be any of the **10 digits** i.e 0 -9 ", "cell_type": "markdown", "metadata": {}}, {"source": "n_input = 784 \nn_classes = 10 ", "cell_type": "code", "execution_count": 3, "outputs": [], "metadata": {}}, {"source": "### 1.3 Define the input and the target(label) Tensors", "cell_type": "markdown", "metadata": {}}, {"source": "x - A placeholder that will hold the input data for training and scoring <br>\ny - A placeholder that will hold the numeric value of the hand written number in the image", "cell_type": "markdown", "metadata": {}}, {"source": "# tf Graph input\nx = tf.placeholder(tf.float32, [None, n_input], name=\"x_input\")\ny = tf.placeholder(tf.float32, [None, n_classes])", "cell_type": "code", "execution_count": 4, "outputs": [], "metadata": {}}, {"source": "### 1.4 Set the convolutional neural network related parameters", "cell_type": "markdown", "metadata": {}}, {"source": "The convolutional neural network that we are going to build requires weights and biases to be initialized for each of the convolution layer in the network. The code below initializes these weights and biases.", "cell_type": "markdown", "metadata": {}}, {"source": "# Initialize each convolution layer's weights, bias and dropout \nweights = {\n    # 5x5 conv, 1 input, 32 outputs\n    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n    # 5x5 conv, 32 inputs, 64 outputs\n    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n    # fully connected, 7*7*64 inputs, 1024 outputs\n    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n    # 1024 inputs, 10 outputs (class prediction)\n    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n}\n\nbiases = {\n    'bc1': tf.Variable(tf.random_normal([32])),\n    'bc2': tf.Variable(tf.random_normal([64])),\n    'bd1': tf.Variable(tf.random_normal([1024])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n\ndropout = 0.75\nlearning_rate = 0.001", "cell_type": "code", "execution_count": 5, "outputs": [], "metadata": {}}, {"source": "### 1.5 Build the model definition", "cell_type": "markdown", "metadata": {}}, {"source": "In Tensorflow, a model is built by building a computational graph. The computational graph is in turn built by defining the nodes. Each node refers to a placeholder or a transformation operation of the input data. The source of input data for a node could either be another node in the graph or from the user specified at time executing the graph. In Tensorflow's terms, each node is called as a Tensor. <br>\n\nIn the cell below, we define the Tensors(nodes) in the graph that implement convolutional neural network architecture.", "cell_type": "markdown", "metadata": {}}, {"source": "# Reshape input picture\nx_trans1 = tf.reshape(x, shape=[-1, 28, 28, 1])\n\n# Convolution Layer -1\nx_conv2d_l1 = tf.nn.conv2d(x_trans1, weights['wc1'], strides=[1, 1, 1, 1], padding='SAME')\nx_w_bias_l1 = tf.nn.bias_add(x_conv2d_l1, biases['bc1'])\nx_relu_l1 = tf.nn.relu(x_w_bias_l1)\nconv1_out = tf.nn.max_pool(x_relu_l1,\n                           ksize=[1, 2, 2, 1],\n                           strides=[1, 2, 2, 1],\n                           padding='SAME')\n\n\n# Convolution Layer -2\nx_conv2d_l2 = tf.nn.conv2d(conv1_out, weights['wc2'], strides=[1, 1, 1, 1], padding='SAME')\nx_w_bias_l2 = tf.nn.bias_add(x_conv2d_l2, biases['bc2'])\nx_relu_l2 = tf.nn.relu(x_w_bias_l2)\nconv2_out = tf.nn.max_pool(x_relu_l2,\n                           ksize=[1, 2, 2, 1],\n                           strides=[1, 2, 2, 1],\n                           padding='SAME')\n\n# Fully connected layer\n# Reshape conv2 output to fit fully connected layer input\nfc1 = tf.reshape(conv2_out, [-1, weights['wd1'].get_shape().as_list()[0]])\nfc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\nfc1 = tf.nn.relu(fc1)\n\n# Apply Dropout\nfc1 = tf.nn.dropout(fc1, dropout)\n\n# Output, class prediction\nconv_out = tf.add(tf.matmul(fc1, weights['out']), biases['out'], name=\"output_tensor\")\n\npredictor = tf.argmax(conv_out, 1, name=\"predictor\")\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=conv_out, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# To Evaluate model\ncorrect_pred = tf.equal(tf.argmax(conv_out, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n", "cell_type": "code", "execution_count": 6, "outputs": [], "metadata": {}}, {"source": "### 1.6 Set parameters for training", "cell_type": "markdown", "metadata": {}}, {"source": "training_iters - Refers to the number of images that we plan to use for training the model. Using more images for training leads to better accuracy of the model <br>\nbatch_size - The training will be performed iteratively on a batch of images. batch_size refers to the number of images that needs to be part of the batch <br>\ndisplay_step  - Refers to the n-th iteration of training after which the training accuracy data will be calculated and displayed", "cell_type": "markdown", "metadata": {}}, {"source": "# Training Parameters\ntraining_iters = 60000 \nbatch_size = 128\ndisplay_step = 10", "cell_type": "code", "execution_count": 7, "outputs": [], "metadata": {}}, {"source": "### 1.7 Initialize a Tensorflow Session to train the model", "cell_type": "markdown", "metadata": {}}, {"source": "Training a model refers to executing the computational graph that holds the model defintion. <br>\nTensorflow uses a C++ backend application to execute the computational graph. The connection to the C++ backend application from Tensorflow's Python runtime is managed by Session object. We hence initialize a Session using the code below ", "cell_type": "markdown", "metadata": {}}, {"source": "# Initializing the variables\ninit = tf.global_variables_initializer()\n# Launch the graph\nsess = tf.Session()\nsess.run(init)", "cell_type": "code", "execution_count": 8, "outputs": [], "metadata": {}}, {"source": "\nstep = 1\n# Keep training until reach max iterations\nwhile step * batch_size < training_iters:\n    batch_x, batch_y = mnist.train.next_batch(batch_size)\n    # Run optimization op (backprop)\n    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n    print(\"Completed batch iteration: \" + str(step*batch_size) )\n    if step % display_step == 0:\n        # Calculate batch loss and accuracy\n        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n                                                          y: batch_y})\n    \n        print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n              \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n              \"{:.5f}\".format(acc))\n    step += 1\nprint(\"Model training finished!\")", "cell_type": "code", "execution_count": 9, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Completed batch iteration: 128\nCompleted batch iteration: 256\nCompleted batch iteration: 384\nCompleted batch iteration: 512\nCompleted batch iteration: 640\nCompleted batch iteration: 768\nCompleted batch iteration: 896\nCompleted batch iteration: 1024\nCompleted batch iteration: 1152\nCompleted batch iteration: 1280\nIter 1280, Minibatch Loss= 44653.765625, Training Accuracy= 0.14844\nCompleted batch iteration: 1408\nCompleted batch iteration: 1536\nCompleted batch iteration: 1664\nCompleted batch iteration: 1792\nCompleted batch iteration: 1920\nCompleted batch iteration: 2048\nCompleted batch iteration: 2176\nCompleted batch iteration: 2304\nCompleted batch iteration: 2432\nCompleted batch iteration: 2560\nIter 2560, Minibatch Loss= 32052.625000, Training Accuracy= 0.30469\nCompleted batch iteration: 2688\nCompleted batch iteration: 2816\nCompleted batch iteration: 2944\nCompleted batch iteration: 3072\nCompleted batch iteration: 3200\nCompleted batch iteration: 3328\nCompleted batch iteration: 3456\nCompleted batch iteration: 3584\nCompleted batch iteration: 3712\nCompleted batch iteration: 3840\nIter 3840, Minibatch Loss= 22396.429688, Training Accuracy= 0.35156\nCompleted batch iteration: 3968\nCompleted batch iteration: 4096\nCompleted batch iteration: 4224\nCompleted batch iteration: 4352\nCompleted batch iteration: 4480\nCompleted batch iteration: 4608\nCompleted batch iteration: 4736\nCompleted batch iteration: 4864\nCompleted batch iteration: 4992\nCompleted batch iteration: 5120\nIter 5120, Minibatch Loss= 14274.168945, Training Accuracy= 0.53125\nCompleted batch iteration: 5248\nCompleted batch iteration: 5376\nCompleted batch iteration: 5504\nCompleted batch iteration: 5632\nCompleted batch iteration: 5760\nCompleted batch iteration: 5888\nCompleted batch iteration: 6016\nCompleted batch iteration: 6144\nCompleted batch iteration: 6272\nCompleted batch iteration: 6400\nIter 6400, Minibatch Loss= 13663.063477, Training Accuracy= 0.55469\nCompleted batch iteration: 6528\nCompleted batch iteration: 6656\nCompleted batch iteration: 6784\nCompleted batch iteration: 6912\nCompleted batch iteration: 7040\nCompleted batch iteration: 7168\nCompleted batch iteration: 7296\nCompleted batch iteration: 7424\nCompleted batch iteration: 7552\nCompleted batch iteration: 7680\nIter 7680, Minibatch Loss= 7859.664551, Training Accuracy= 0.65625\nCompleted batch iteration: 7808\nCompleted batch iteration: 7936\nCompleted batch iteration: 8064\nCompleted batch iteration: 8192\nCompleted batch iteration: 8320\nCompleted batch iteration: 8448\nCompleted batch iteration: 8576\nCompleted batch iteration: 8704\nCompleted batch iteration: 8832\nCompleted batch iteration: 8960\nIter 8960, Minibatch Loss= 6967.487305, Training Accuracy= 0.70312\nCompleted batch iteration: 9088\nCompleted batch iteration: 9216\nCompleted batch iteration: 9344\nCompleted batch iteration: 9472\nCompleted batch iteration: 9600\nCompleted batch iteration: 9728\nCompleted batch iteration: 9856\nCompleted batch iteration: 9984\nCompleted batch iteration: 10112\nCompleted batch iteration: 10240\nIter 10240, Minibatch Loss= 6350.914062, Training Accuracy= 0.69531\nCompleted batch iteration: 10368\nCompleted batch iteration: 10496\nCompleted batch iteration: 10624\nCompleted batch iteration: 10752\nCompleted batch iteration: 10880\nCompleted batch iteration: 11008\nCompleted batch iteration: 11136\nCompleted batch iteration: 11264\nCompleted batch iteration: 11392\nCompleted batch iteration: 11520\nIter 11520, Minibatch Loss= 6416.614258, Training Accuracy= 0.70312\nCompleted batch iteration: 11648\nCompleted batch iteration: 11776\nCompleted batch iteration: 11904\nCompleted batch iteration: 12032\nCompleted batch iteration: 12160\nCompleted batch iteration: 12288\nCompleted batch iteration: 12416\nCompleted batch iteration: 12544\nCompleted batch iteration: 12672\nCompleted batch iteration: 12800\nIter 12800, Minibatch Loss= 4929.777344, Training Accuracy= 0.75781\nCompleted batch iteration: 12928\nCompleted batch iteration: 13056\nCompleted batch iteration: 13184\nCompleted batch iteration: 13312\nCompleted batch iteration: 13440\nCompleted batch iteration: 13568\nCompleted batch iteration: 13696\nCompleted batch iteration: 13824\nCompleted batch iteration: 13952\nCompleted batch iteration: 14080\nIter 14080, Minibatch Loss= 4925.335938, Training Accuracy= 0.75000\nCompleted batch iteration: 14208\nCompleted batch iteration: 14336\nCompleted batch iteration: 14464\nCompleted batch iteration: 14592\nCompleted batch iteration: 14720\nCompleted batch iteration: 14848\nCompleted batch iteration: 14976\nCompleted batch iteration: 15104\nCompleted batch iteration: 15232\nCompleted batch iteration: 15360\nIter 15360, Minibatch Loss= 4323.862793, Training Accuracy= 0.77344\nCompleted batch iteration: 15488\nCompleted batch iteration: 15616\nCompleted batch iteration: 15744\nCompleted batch iteration: 15872\nCompleted batch iteration: 16000\nCompleted batch iteration: 16128\nCompleted batch iteration: 16256\nCompleted batch iteration: 16384\nCompleted batch iteration: 16512\nCompleted batch iteration: 16640\nIter 16640, Minibatch Loss= 2219.532227, Training Accuracy= 0.86719\nCompleted batch iteration: 16768\nCompleted batch iteration: 16896\nCompleted batch iteration: 17024\nCompleted batch iteration: 17152\nCompleted batch iteration: 17280\nCompleted batch iteration: 17408\nCompleted batch iteration: 17536\nCompleted batch iteration: 17664\nCompleted batch iteration: 17792\nCompleted batch iteration: 17920\nIter 17920, Minibatch Loss= 3257.096436, Training Accuracy= 0.82031\nCompleted batch iteration: 18048\nCompleted batch iteration: 18176\nCompleted batch iteration: 18304\nCompleted batch iteration: 18432\nCompleted batch iteration: 18560\nCompleted batch iteration: 18688\nCompleted batch iteration: 18816\nCompleted batch iteration: 18944\nCompleted batch iteration: 19072\nCompleted batch iteration: 19200\nIter 19200, Minibatch Loss= 4111.493652, Training Accuracy= 0.81250\nCompleted batch iteration: 19328\nCompleted batch iteration: 19456\nCompleted batch iteration: 19584\nCompleted batch iteration: 19712\nCompleted batch iteration: 19840\nCompleted batch iteration: 19968\nCompleted batch iteration: 20096\nCompleted batch iteration: 20224\nCompleted batch iteration: 20352\nCompleted batch iteration: 20480\nIter 20480, Minibatch Loss= 4292.917480, Training Accuracy= 0.82812\nCompleted batch iteration: 20608\nCompleted batch iteration: 20736\nCompleted batch iteration: 20864\nCompleted batch iteration: 20992\nCompleted batch iteration: 21120\nCompleted batch iteration: 21248\nCompleted batch iteration: 21376\nCompleted batch iteration: 21504\nCompleted batch iteration: 21632\nCompleted batch iteration: 21760\nIter 21760, Minibatch Loss= 2822.654785, Training Accuracy= 0.84375\nCompleted batch iteration: 21888\nCompleted batch iteration: 22016\nCompleted batch iteration: 22144\nCompleted batch iteration: 22272\nCompleted batch iteration: 22400\nCompleted batch iteration: 22528\nCompleted batch iteration: 22656\nCompleted batch iteration: 22784\nCompleted batch iteration: 22912\nCompleted batch iteration: 23040\nIter 23040, Minibatch Loss= 2252.683105, Training Accuracy= 0.82031\nCompleted batch iteration: 23168\nCompleted batch iteration: 23296\nCompleted batch iteration: 23424\nCompleted batch iteration: 23552\nCompleted batch iteration: 23680\nCompleted batch iteration: 23808\nCompleted batch iteration: 23936\nCompleted batch iteration: 24064\nCompleted batch iteration: 24192\nCompleted batch iteration: 24320\nIter 24320, Minibatch Loss= 1981.265381, Training Accuracy= 0.83594\nCompleted batch iteration: 24448\nCompleted batch iteration: 24576\nCompleted batch iteration: 24704\nCompleted batch iteration: 24832\nCompleted batch iteration: 24960\nCompleted batch iteration: 25088\nCompleted batch iteration: 25216\nCompleted batch iteration: 25344\nCompleted batch iteration: 25472\nCompleted batch iteration: 25600\nIter 25600, Minibatch Loss= 4126.081055, Training Accuracy= 0.78906\nCompleted batch iteration: 25728\nCompleted batch iteration: 25856\nCompleted batch iteration: 25984\nCompleted batch iteration: 26112\nCompleted batch iteration: 26240\nCompleted batch iteration: 26368\nCompleted batch iteration: 26496\nCompleted batch iteration: 26624\nCompleted batch iteration: 26752\nCompleted batch iteration: 26880\nIter 26880, Minibatch Loss= 2405.273682, Training Accuracy= 0.85156\n"}, {"output_type": "stream", "name": "stdout", "text": "Completed batch iteration: 27008\nCompleted batch iteration: 27136\nCompleted batch iteration: 27264\nCompleted batch iteration: 27392\nCompleted batch iteration: 27520\nCompleted batch iteration: 27648\nCompleted batch iteration: 27776\nCompleted batch iteration: 27904\nCompleted batch iteration: 28032\nCompleted batch iteration: 28160\nIter 28160, Minibatch Loss= 2945.165527, Training Accuracy= 0.80469\nCompleted batch iteration: 28288\nCompleted batch iteration: 28416\nCompleted batch iteration: 28544\nCompleted batch iteration: 28672\nCompleted batch iteration: 28800\nCompleted batch iteration: 28928\nCompleted batch iteration: 29056\nCompleted batch iteration: 29184\nCompleted batch iteration: 29312\nCompleted batch iteration: 29440\nIter 29440, Minibatch Loss= 2996.718262, Training Accuracy= 0.86719\nCompleted batch iteration: 29568\nCompleted batch iteration: 29696\nCompleted batch iteration: 29824\nCompleted batch iteration: 29952\nCompleted batch iteration: 30080\nCompleted batch iteration: 30208\nCompleted batch iteration: 30336\nCompleted batch iteration: 30464\nCompleted batch iteration: 30592\nCompleted batch iteration: 30720\nIter 30720, Minibatch Loss= 1936.788818, Training Accuracy= 0.83594\nCompleted batch iteration: 30848\nCompleted batch iteration: 30976\nCompleted batch iteration: 31104\nCompleted batch iteration: 31232\nCompleted batch iteration: 31360\nCompleted batch iteration: 31488\nCompleted batch iteration: 31616\nCompleted batch iteration: 31744\nCompleted batch iteration: 31872\nCompleted batch iteration: 32000\nIter 32000, Minibatch Loss= 2603.742676, Training Accuracy= 0.84375\nCompleted batch iteration: 32128\nCompleted batch iteration: 32256\nCompleted batch iteration: 32384\nCompleted batch iteration: 32512\nCompleted batch iteration: 32640\nCompleted batch iteration: 32768\nCompleted batch iteration: 32896\nCompleted batch iteration: 33024\nCompleted batch iteration: 33152\nCompleted batch iteration: 33280\nIter 33280, Minibatch Loss= 2122.693359, Training Accuracy= 0.86719\nCompleted batch iteration: 33408\nCompleted batch iteration: 33536\nCompleted batch iteration: 33664\nCompleted batch iteration: 33792\nCompleted batch iteration: 33920\nCompleted batch iteration: 34048\nCompleted batch iteration: 34176\nCompleted batch iteration: 34304\nCompleted batch iteration: 34432\nCompleted batch iteration: 34560\nIter 34560, Minibatch Loss= 825.833618, Training Accuracy= 0.90625\nCompleted batch iteration: 34688\nCompleted batch iteration: 34816\nCompleted batch iteration: 34944\nCompleted batch iteration: 35072\nCompleted batch iteration: 35200\nCompleted batch iteration: 35328\nCompleted batch iteration: 35456\nCompleted batch iteration: 35584\nCompleted batch iteration: 35712\nCompleted batch iteration: 35840\nIter 35840, Minibatch Loss= 2448.950684, Training Accuracy= 0.84375\nCompleted batch iteration: 35968\nCompleted batch iteration: 36096\nCompleted batch iteration: 36224\nCompleted batch iteration: 36352\nCompleted batch iteration: 36480\nCompleted batch iteration: 36608\nCompleted batch iteration: 36736\nCompleted batch iteration: 36864\nCompleted batch iteration: 36992\nCompleted batch iteration: 37120\nIter 37120, Minibatch Loss= 2268.870850, Training Accuracy= 0.82812\nCompleted batch iteration: 37248\nCompleted batch iteration: 37376\nCompleted batch iteration: 37504\nCompleted batch iteration: 37632\nCompleted batch iteration: 37760\nCompleted batch iteration: 37888\nCompleted batch iteration: 38016\nCompleted batch iteration: 38144\nCompleted batch iteration: 38272\nCompleted batch iteration: 38400\nIter 38400, Minibatch Loss= 1958.814453, Training Accuracy= 0.92188\nCompleted batch iteration: 38528\nCompleted batch iteration: 38656\nCompleted batch iteration: 38784\nCompleted batch iteration: 38912\nCompleted batch iteration: 39040\nCompleted batch iteration: 39168\nCompleted batch iteration: 39296\nCompleted batch iteration: 39424\nCompleted batch iteration: 39552\nCompleted batch iteration: 39680\nIter 39680, Minibatch Loss= 660.082520, Training Accuracy= 0.91406\nCompleted batch iteration: 39808\nCompleted batch iteration: 39936\nCompleted batch iteration: 40064\nCompleted batch iteration: 40192\nCompleted batch iteration: 40320\nCompleted batch iteration: 40448\nCompleted batch iteration: 40576\nCompleted batch iteration: 40704\nCompleted batch iteration: 40832\nCompleted batch iteration: 40960\nIter 40960, Minibatch Loss= 1127.785645, Training Accuracy= 0.93750\nCompleted batch iteration: 41088\nCompleted batch iteration: 41216\nCompleted batch iteration: 41344\nCompleted batch iteration: 41472\nCompleted batch iteration: 41600\nCompleted batch iteration: 41728\nCompleted batch iteration: 41856\nCompleted batch iteration: 41984\nCompleted batch iteration: 42112\nCompleted batch iteration: 42240\nIter 42240, Minibatch Loss= 1014.650574, Training Accuracy= 0.89844\nCompleted batch iteration: 42368\nCompleted batch iteration: 42496\nCompleted batch iteration: 42624\nCompleted batch iteration: 42752\nCompleted batch iteration: 42880\nCompleted batch iteration: 43008\nCompleted batch iteration: 43136\nCompleted batch iteration: 43264\nCompleted batch iteration: 43392\nCompleted batch iteration: 43520\nIter 43520, Minibatch Loss= 680.080322, Training Accuracy= 0.93750\nCompleted batch iteration: 43648\nCompleted batch iteration: 43776\nCompleted batch iteration: 43904\nCompleted batch iteration: 44032\nCompleted batch iteration: 44160\nCompleted batch iteration: 44288\nCompleted batch iteration: 44416\nCompleted batch iteration: 44544\nCompleted batch iteration: 44672\nCompleted batch iteration: 44800\nIter 44800, Minibatch Loss= 860.581970, Training Accuracy= 0.90625\nCompleted batch iteration: 44928\nCompleted batch iteration: 45056\nCompleted batch iteration: 45184\nCompleted batch iteration: 45312\nCompleted batch iteration: 45440\nCompleted batch iteration: 45568\nCompleted batch iteration: 45696\nCompleted batch iteration: 45824\nCompleted batch iteration: 45952\nCompleted batch iteration: 46080\nIter 46080, Minibatch Loss= 839.567627, Training Accuracy= 0.92188\nCompleted batch iteration: 46208\nCompleted batch iteration: 46336\nCompleted batch iteration: 46464\nCompleted batch iteration: 46592\nCompleted batch iteration: 46720\nCompleted batch iteration: 46848\nCompleted batch iteration: 46976\nCompleted batch iteration: 47104\nCompleted batch iteration: 47232\nCompleted batch iteration: 47360\nIter 47360, Minibatch Loss= 1499.093384, Training Accuracy= 0.89062\nCompleted batch iteration: 47488\nCompleted batch iteration: 47616\nCompleted batch iteration: 47744\nCompleted batch iteration: 47872\nCompleted batch iteration: 48000\nCompleted batch iteration: 48128\nCompleted batch iteration: 48256\nCompleted batch iteration: 48384\nCompleted batch iteration: 48512\nCompleted batch iteration: 48640\nIter 48640, Minibatch Loss= 1315.224365, Training Accuracy= 0.87500\nCompleted batch iteration: 48768\nCompleted batch iteration: 48896\nCompleted batch iteration: 49024\nCompleted batch iteration: 49152\nCompleted batch iteration: 49280\nCompleted batch iteration: 49408\nCompleted batch iteration: 49536\nCompleted batch iteration: 49664\nCompleted batch iteration: 49792\nCompleted batch iteration: 49920\nIter 49920, Minibatch Loss= 1395.882324, Training Accuracy= 0.89844\nCompleted batch iteration: 50048\nCompleted batch iteration: 50176\nCompleted batch iteration: 50304\nCompleted batch iteration: 50432\nCompleted batch iteration: 50560\nCompleted batch iteration: 50688\nCompleted batch iteration: 50816\nCompleted batch iteration: 50944\nCompleted batch iteration: 51072\nCompleted batch iteration: 51200\nIter 51200, Minibatch Loss= 574.512451, Training Accuracy= 0.93750\nCompleted batch iteration: 51328\nCompleted batch iteration: 51456\nCompleted batch iteration: 51584\nCompleted batch iteration: 51712\nCompleted batch iteration: 51840\nCompleted batch iteration: 51968\nCompleted batch iteration: 52096\nCompleted batch iteration: 52224\nCompleted batch iteration: 52352\nCompleted batch iteration: 52480\nIter 52480, Minibatch Loss= 1294.932739, Training Accuracy= 0.91406\nCompleted batch iteration: 52608\nCompleted batch iteration: 52736\nCompleted batch iteration: 52864\nCompleted batch iteration: 52992\nCompleted batch iteration: 53120\nCompleted batch iteration: 53248\nCompleted batch iteration: 53376\nCompleted batch iteration: 53504\n"}, {"output_type": "stream", "name": "stdout", "text": "Completed batch iteration: 53632\nCompleted batch iteration: 53760\nIter 53760, Minibatch Loss= 769.404541, Training Accuracy= 0.93750\nCompleted batch iteration: 53888\nCompleted batch iteration: 54016\nCompleted batch iteration: 54144\nCompleted batch iteration: 54272\nCompleted batch iteration: 54400\nCompleted batch iteration: 54528\nCompleted batch iteration: 54656\nCompleted batch iteration: 54784\nCompleted batch iteration: 54912\nCompleted batch iteration: 55040\nIter 55040, Minibatch Loss= 807.323486, Training Accuracy= 0.92188\nCompleted batch iteration: 55168\nCompleted batch iteration: 55296\nCompleted batch iteration: 55424\nCompleted batch iteration: 55552\nCompleted batch iteration: 55680\nCompleted batch iteration: 55808\nCompleted batch iteration: 55936\nCompleted batch iteration: 56064\nCompleted batch iteration: 56192\nCompleted batch iteration: 56320\nIter 56320, Minibatch Loss= 2225.143066, Training Accuracy= 0.90625\nCompleted batch iteration: 56448\nCompleted batch iteration: 56576\nCompleted batch iteration: 56704\nCompleted batch iteration: 56832\nCompleted batch iteration: 56960\nCompleted batch iteration: 57088\nCompleted batch iteration: 57216\nCompleted batch iteration: 57344\nCompleted batch iteration: 57472\nCompleted batch iteration: 57600\nIter 57600, Minibatch Loss= 1380.129639, Training Accuracy= 0.89844\nCompleted batch iteration: 57728\nCompleted batch iteration: 57856\nCompleted batch iteration: 57984\nCompleted batch iteration: 58112\nCompleted batch iteration: 58240\nCompleted batch iteration: 58368\nCompleted batch iteration: 58496\nCompleted batch iteration: 58624\nCompleted batch iteration: 58752\nCompleted batch iteration: 58880\nIter 58880, Minibatch Loss= 2019.631714, Training Accuracy= 0.87500\nCompleted batch iteration: 59008\nCompleted batch iteration: 59136\nCompleted batch iteration: 59264\nCompleted batch iteration: 59392\nCompleted batch iteration: 59520\nCompleted batch iteration: 59648\nCompleted batch iteration: 59776\nCompleted batch iteration: 59904\nModel training finished!\n"}], "metadata": {}}, {"source": "** We have now trained a Tensorflow model. As a next step we need to persist this model in WML Repository **", "cell_type": "markdown", "metadata": {}}, {"source": "# 2.0 Save the trained Tensorflow model in WML Repository using Repository's Python Client", "cell_type": "markdown", "metadata": {}}, {"source": "### 2.1 Create the signature of the tensors that will be required for scoring. ", "cell_type": "markdown", "metadata": {}}, {"source": "Signature refers to the information about the Tensors that hold the input data and the output data for scoring. This signature will be used at the time of scoring using the WML Online Deployment and Scoring service. <br>\n\nAs per our model definition, <br> \nthe Tensor - \"x\" is the placeholder that holds the input data of the model and <br>\nthe Tensor - \"predictor\" is the node that holds the predicted value.", "cell_type": "markdown", "metadata": {}}, {"source": "### P.S : \n** This is a mandatory requirement for scoring the model. Hence this data should be provided while saving the model. **", "cell_type": "markdown", "metadata": {}}, {"source": "classification_inputs = tf.saved_model.utils.build_tensor_info(x)\nclassification_outputs_classes = tf.saved_model.utils.build_tensor_info(predictor)\n\nclassification_signature = (\n      tf.saved_model.signature_def_utils.build_signature_def(\n          inputs={\n              tf.saved_model.signature_constants.CLASSIFY_INPUTS:\n                  classification_inputs\n          },\n          outputs={\n              tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES:\n                  classification_outputs_classes\n          },\n          method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME))\n\nprint(\"classification_signature content:\")\nprint(classification_signature)\n\nlegacy_op_init = tf.group(tf.tables_initializer(), name='legacy_init_op')\n", "cell_type": "code", "execution_count": 10, "outputs": [{"output_type": "stream", "name": "stdout", "text": "classification_signature content:\ninputs {\n  key: \"inputs\"\n  value {\n    name: \"x_input:0\"\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: -1\n      }\n      dim {\n        size: 784\n      }\n    }\n  }\n}\noutputs {\n  key: \"classes\"\n  value {\n    name: \"predictor:0\"\n    dtype: DT_INT64\n    tensor_shape {\n      dim {\n        size: -1\n      }\n    }\n  }\n}\nmethod_name: \"tensorflow/serving/classify\"\n\n"}], "metadata": {}}, {"source": "### 2.2 Save the model using WML Repository's Python client ", "cell_type": "markdown", "metadata": {}}, {"source": "First, we must import client libraries.", "cell_type": "markdown", "metadata": {}}, {"source": "\nimport sys, time\nfrom repository_v3.mlrepository import MetaNames, MetaProps\nfrom repository_v3.mlrepositoryclient import MLRepositoryClient\nfrom repository_v3.mlrepositoryartifact import MLRepositoryArtifact\n", "cell_type": "code", "execution_count": 11, "outputs": [], "metadata": {}}, {"source": "#### 2.2.1 Provide WML instance credentials and authenticate", "cell_type": "markdown", "metadata": {}}, {"source": "Authenticate to Watson Machine Learning service on Bluemix.\n\n**Action:** Add authentication information from your instance of Watson Machine Learning service here", "cell_type": "markdown", "metadata": {}}, {"source": "# WML Instance details\n\n# SVT\n#service_url = \"https://ibm-watson-ml-svt.stage1.mybluemix.net\"\nservice_url = \"https://ibm-watson-ml.mybluemix.net\"\nuser = \"10c5f8f1-ab84-4501-91e1-dda53fc438cd\"\npassword =  \"24ec7cc5-250f-4bdf-8a94-1200be3d1cb7\"\ninstance_id =  \"d8b98cb2-cd06-4740-8467-def50eca91f9\"", "cell_type": "code", "execution_count": 12, "outputs": [], "metadata": {}}, {"source": "ml_repository_client = MLRepositoryClient(service_url)\nml_repository_client.authorize(user, password)", "cell_type": "code", "execution_count": 13, "outputs": [], "metadata": {}}, {"source": "#### 2.2.2 Create a WML Artifact", "cell_type": "markdown", "metadata": {}}, {"source": "Define the metadata about the model that wish to persist along with the model. ", "cell_type": "markdown", "metadata": {}}, {"source": "tf_model_name = 'k_tf_mnist_10203'\ntf_model_metadata = {\n    MetaNames.DESCRIPTION: \"Tensorflow model for predecting Hand-written digits\",\n    MetaNames.AUTHOR_EMAIL:\"krishna@in.ibm.com\",\n    MetaNames.AUTHOR_NAME: \"Krishna\",\n}", "cell_type": "code", "execution_count": 14, "outputs": [], "metadata": {}}, {"source": "Create a WML Repository artifact by specifying the session object that contains the graph of the model that we wish to save in WML Repository and the scoring signature of the model. We also specify other metadata that we want to save along the model.", "cell_type": "markdown", "metadata": {}}, {"source": "\ntf_model_artifact = MLRepositoryArtifact(\n    sess,\n    signature_def_map={\n        'predict_images': classification_signature\n      },\n    legacy_init_op=legacy_op_init,\n    name=tf_model_name,\n    meta_props=MetaProps(tf_model_metadata.copy())\n)\n", "cell_type": "code", "execution_count": 15, "outputs": [], "metadata": {}}, {"source": "#### 2.3 Save the model to WML Repository", "cell_type": "markdown", "metadata": {}}, {"source": "The code below serializes the model artifact that contains reference to the Session object and the related details and saves it in WML Repository as a compressed tar ball. <br>\nThe API returns a bunch of metadata that was created as part of saving the model.", "cell_type": "markdown", "metadata": {}}, {"source": "saved_model = ml_repository_client.models.save(tf_model_artifact)", "cell_type": "code", "execution_count": 16, "outputs": [], "metadata": {}}, {"source": "Display few metadata of our interest that was generated as part of saving the model to the WML Repository. <br>\n\nmodelVersionUrl displayed in output of the cell below refers to the WML Repository URL that points to the saved model.", "cell_type": "markdown", "metadata": {}}, {"source": "model_uid  = saved_model.uid\nmodel_ver_url  = saved_model.meta.prop(\"modelVersionUrl\")\nprint(\"ModelType: \" + saved_model.meta.prop(\"frameworkName\") + \"-\" + saved_model.meta.prop(\"frameworkVersion\"))\nprint(\"ModelId: \" + saved_model.uid)\nprint(\"modelVersionUrl: \" + saved_model.meta.prop(\"modelVersionUrl\"))", "cell_type": "code", "execution_count": 17, "outputs": [{"output_type": "stream", "name": "stdout", "text": "ModelType: tensorflow-1.2\nModelId: 3a982151-54b5-462f-9006-2243587c6af1\nmodelVersionUrl: https://ibm-watson-ml.mybluemix.net/v3/ml_assets/models/3a982151-54b5-462f-9006-2243587c6af1/versions/53e27596-1d29-4583-bd2c-503b7cd80e8c\n"}], "metadata": {}}, {"source": "As we now have persisted a trained model in WML Repository, we are ready to deploy and score using this model. The deployment and scoring functionality is explained in the notebook named \"WML_TF_Serving_Using_Onllike_Deploy_Scoring_Service\" in this project.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "!curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header 'Authorization: Bearer eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJ0ZW5hbnRJZCI6ImQ4Yjk4Y2IyLWNkMDYtNDc0MC04NDY3LWRlZjUwZWNhOTFmOSIsImluc3RhbmNlSWQiOiJkOGI5OGNiMi1jZDA2LTQ3NDAtODQ2Ny1kZWY1MGVjYTkxZjkiLCJwbGFuSWQiOiIzZjZhY2Y0My1lZGU4LTQxM2EtYWM2OS1mOGFmM2JiMGNiZmUiLCJyZWdpb24iOiJ1cy1zb3V0aCIsInVzZXJJZCI6IjEwYzVmOGYxLWFiODQtNDUwMS05MWUxLWRkYTUzZmM0MzhjZCIsImlzcyI6Imh0dHBzOi8vaWJtLXdhdHNvbi1tbC5teWJsdWVtaXgubmV0L3YzL2lkZW50aXR5IiwiaWF0IjoxNTE2OTYyNzkzLCJleHAiOjE1MTY5OTE1OTMsImFwcGxpY2F0aW9uSWQiOiI5NjQ2YTA0NC03YmUyLTQyYTYtYmIxMy0zNjUyYzI0NDYyMTQifQ.WtB_Vx7KQW1AFMfdbZV4xEA2NgOJun2OoeLOLCaUNPMmlrdGmZw00E6kT96F8KvwhtfTPOUzBx46bqg1Y1nZ30W2XtjsenNNQoDZnfJxySvzc2pC0ixW3KhfFujG857frJj1cbu3WPhd0PQMeojuEbD2GOod_N339rX-mcZHhctkw0cS-sgTUilggz0R8gS9C2bmk_gWl05xYW1M4Ayb-4bbQnvt6rumdJjTxk2Sf_iH0plDmVNblUNA5Jdb0IYjZYg7sbbZW-SXJzqwolc-6Nelne5fvlouUuujuFkboxJpNeCoyNusexOnnRaM2DANZwEv7RlwnxniEGgrxOC8Wg' -d '{\"inputs\":[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19607844948768616, 0.8784314393997192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27450981736183167, 0.11372549831867218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4745098352432251, 0.9058824181556702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5803921818733215, 0.658823549747467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01568627543747425, 0.7647059559822083, 0.9058824181556702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3764706254005432, 0.8235294818878174, 0.04313725605607033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2705882489681244, 0.988235354423523, 0.5254902243614197, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44705885648727417, 0.988235354423523, 0.08235294371843338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1764705926179886, 0.9254902601242065, 0.8509804606437683, 0.0470588281750679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7529412508010864, 0.988235354423523, 0.08235294371843338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.658823549747467, 0.9686275124549866, 0.20784315466880798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07058823853731155, 1.0, 0.9921569228172302, 0.08235294371843338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3294117748737335, 0.9490196704864502, 0.8274510502815247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5529412031173706, 0.9921569228172302, 0.7411764860153198, 0.019607843831181526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6627451181411743, 0.988235354423523, 0.41568630933761597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125490203499794, 0.9098039865493774, 0.9803922176361084, 0.25882354378700256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05882353335618973, 0.8823530077934265, 0.988235354423523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5254902243614197, 0.988235354423523, 0.8274510502815247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08627451211214066, 0.988235354423523, 0.6431372761726379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6627451181411743, 0.988235354423523, 0.6549019813537598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03529411926865578, 0.8000000715255737, 0.8196079134941101, 0.07058823853731155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08627451211214066, 0.9921569228172302, 0.9921569228172302, 0.41960787773132324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6627451181411743, 0.988235354423523, 0.7803922295570374, 0.3333333432674408, 0.3333333432674408, 0.3333333432674408, 0.3333333432674408, 0.5058823823928833, 0.6431372761726379, 0.7647059559822083, 0.988235354423523, 0.988235354423523, 0.41568630933761597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16078431904315948, 0.6666666865348816, 0.960784375667572, 0.988235354423523, 0.988235354423523, 0.988235354423523, 0.988235354423523, 0.9098039865493774, 0.9058824181556702, 0.9843137860298157, 0.988235354423523, 0.988235354423523, 0.03529411926865578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19215688109397888, 0.3294117748737335, 0.3294117748737335, 0.3294117748737335, 0.3294117748737335, 0.0, 0.0, 0.6313725709915161, 0.988235354423523, 0.988235354423523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49803924560546875, 0.988235354423523, 0.988235354423523, 0.1764705926179886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.501960813999176, 0.9921569228172302, 0.9921569228172302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49803924560546875, 0.988235354423523, 0.988235354423523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.529411792755127, 0.988235354423523, 0.9568628072738647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9098039865493774, 0.9254902601242065, 0.43529415130615234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7019608020782471, 0.25882354378700256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}' 'https://ibm-watson-ml.mybluemix.net/v3/wml_instances/d8b98cb2-cd06-4740-8467-def50eca91f9/published_models/3a982151-54b5-462f-9006-2243587c6af1/deployments/1aa6e50c-ce52-495b-9ac5-2adad19d07ac/online'\n", "cell_type": "code", "execution_count": 40, "outputs": [{"output_type": "stream", "name": "stdout", "text": "{\"classes\":[4]}"}], "metadata": {}}, {"source": "import json\nimport numpy as np\nx,y = mnist.train.next_batch(batch_size)\njson.dumps(np.array(x[0]).tolist())", "cell_type": "code", "execution_count": 36, "outputs": [{"output_type": "execute_result", "metadata": {}, "data": {"text/plain": "'[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1098039299249649, 0.250980406999588, 0.250980406999588, 0.01568627543747425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5254902243614197, 0.43137258291244507, 0.6235294342041016, 0.8000000715255737, 0.9372549653053284, 0.9372549653053284, 0.960784375667572, 0.9960784912109375, 0.9960784912109375, 0.8392157554626465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13333334028720856, 0.6666666865348816, 0.9764706492424011, 0.9372549653053284, 0.9372549653053284, 0.9372549653053284, 0.9372549653053284, 0.6235294342041016, 0.6235294342041016, 0.6235294342041016, 0.27450981736183167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3176470696926117, 0.9647059440612793, 0.8196079134941101, 0.2549019753932953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011764707043766975, 0.46274513006210327, 0.960784375667572, 0.9960784912109375, 0.29411765933036804, 0.1764705926179886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41568630933761597, 0.9960784912109375, 0.9960784912109375, 0.9960784912109375, 0.9960784912109375, 0.9921569228172302, 0.8352941870689392, 0.26274511218070984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3764706254005432, 0.6823529601097107, 0.6823529601097107, 0.4117647409439087, 0.37254902720451355, 0.5882353186607361, 0.917647123336792, 0.9843137860298157, 0.11764706671237946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5529412031173706, 0.988235354423523, 0.11764706671237946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06666667014360428, 0.4235294461250305, 0.05882353335618973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607843831181526, 0.8235294818878174, 0.8078432083129883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7843137979507446, 0.9725490808486938, 0.21176472306251526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32549020648002625, 0.8862745761871338, 0.30588236451148987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7450980544090271, 0.9960784912109375, 0.874509871006012, 0.4862745404243469, 0.0941176563501358, 0.0, 0.0, 0.0, 0.2196078598499298, 0.44705885648727417, 1.0, 0.5176470875740051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16078431904315948, 0.917647123336792, 0.9960784912109375, 0.9960784912109375, 0.9568628072738647, 0.9372549653053284, 0.9372549653053284, 0.9372549653053284, 0.988235354423523, 0.9960784912109375, 0.5686274766921997, 0.0941176563501358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13725490868091583, 0.6235294342041016, 0.9372549653053284, 0.9372549653053284, 0.9372549653053284, 0.9372549653053284, 0.8784314393997192, 0.545098066329956, 0.019607843831181526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]'"}, "execution_count": 36}], "metadata": {}}, {"source": "!curl -X GET --header 'Accept: application/json' --header 'Authorization: Bearer eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJ0ZW5hbnRJZCI6ImQ4Yjk4Y2IyLWNkMDYtNDc0MC04NDY3LWRlZjUwZWNhOTFmOSIsImluc3RhbmNlSWQiOiJkOGI5OGNiMi1jZDA2LTQ3NDAtODQ2Ny1kZWY1MGVjYTkxZjkiLCJwbGFuSWQiOiIzZjZhY2Y0My1lZGU4LTQxM2EtYWM2OS1mOGFmM2JiMGNiZmUiLCJyZWdpb24iOiJ1cy1zb3V0aCIsInVzZXJJZCI6IjEwYzVmOGYxLWFiODQtNDUwMS05MWUxLWRkYTUzZmM0MzhjZCIsImlzcyI6Imh0dHBzOi8vaWJtLXdhdHNvbi1tbC5teWJsdWVtaXgubmV0L3YzL2lkZW50aXR5IiwiaWF0IjoxNTE2OTYyNzkzLCJleHAiOjE1MTY5OTE1OTMsImFwcGxpY2F0aW9uSWQiOiI5NjQ2YTA0NC03YmUyLTQyYTYtYmIxMy0zNjUyYzI0NDYyMTQifQ.WtB_Vx7KQW1AFMfdbZV4xEA2NgOJun2OoeLOLCaUNPMmlrdGmZw00E6kT96F8KvwhtfTPOUzBx46bqg1Y1nZ30W2XtjsenNNQoDZnfJxySvzc2pC0ixW3KhfFujG857frJj1cbu3WPhd0PQMeojuEbD2GOod_N339rX-mcZHhctkw0cS-sgTUilggz0R8gS9C2bmk_gWl05xYW1M4Ayb-4bbQnvt6rumdJjTxk2Sf_iH0plDmVNblUNA5Jdb0IYjZYg7sbbZW-SXJzqwolc-6Nelne5fvlouUuujuFkboxJpNeCoyNusexOnnRaM2DANZwEv7RlwnxniEGgrxOC8Wg' 'https://ibm-watson-ml.mybluemix.net/v3/wml_instances/d8b98cb2-cd06-4740-8467-def50eca91f9/published_models/3a982151-54b5-462f-9006-2243587c6af1/deployments/1aa6e50c-ce52-495b-9ac5-2adad19d07ac'\n", "cell_type": "code", "execution_count": 26, "outputs": [{"output_type": "stream", "name": "stdout", "text": "{\"metadata\":{\"guid\":\"1aa6e50c-ce52-495b-9ac5-2adad19d07ac\",\"url\":\"https://ibm-watson-ml.mybluemix.net/v3/wml_instances/d8b98cb2-cd06-4740-8467-def50eca91f9/published_models/3a982151-54b5-462f-9006-2243587c6af1/deployments/1aa6e50c-ce52-495b-9ac5-2adad19d07ac\",\"created_at\":\"2018-01-26T10:19:17.639Z\",\"modified_at\":\"2018-01-26T10:19:25.978Z\"},\"entity\":{\"runtime_environment\":\"python-3.5\",\"name\":\"mnist deploy\",\"scoring_url\":\"https://ibm-watson-ml.mybluemix.net/v3/wml_instances/d8b98cb2-cd06-4740-8467-def50eca91f9/published_models/3a982151-54b5-462f-9006-2243587c6af1/deployments/1aa6e50c-ce52-495b-9ac5-2adad19d07ac/online\",\"published_model\":{\"author\":{\"name\":\"Krishna\",\"email\":\"krishna@in.ibm.com\"},\"name\":\"k_tf_mnist_10203\",\"url\":\"https://ibm-watson-ml.mybluemix.net/v3/wml_instances/d8b98cb2-cd06-4740-8467-def50eca91f9/published_models/3a982151-54b5-462f-9006-2243587c6af1\",\"guid\":\"3a982151-54b5-462f-9006-2243587c6af1\",\"description\":\"Tensorflow model for predecting Hand-written digits\",\"created_at\":\"2018-01-26T10:19:17.604Z\"},\"model_type\":\"tensorflow-1.2\",\"status\":\"ACTIVE\",\"type\":\"online\",\"deployed_version\":{\"url\":\"https://ibm-watson-ml.mybluemix.net/v3/ml_assets/models/3a982151-54b5-462f-9006-2243587c6af1/versions/53e27596-1d29-4583-bd2c-503b7cd80e8c\",\"guid\":\"53e27596-1d29-4583-bd2c-503b7cd80e8c\"}}}"}], "metadata": {}}, {"source": "image1 = mnist.test.images[45,].tolist()\nimage2 = mnist.test.images[4,].tolist()", "cell_type": "code", "execution_count": 38, "outputs": [], "metadata": {}}, {"source": "scoring_data = {'inputs': [image1, image2]}", "cell_type": "code", "execution_count": 39, "outputs": [], "metadata": {}}, {"source": "", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3.5 (Experimental) with Spark 2.0", "name": "python3-spark20", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.2", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}}